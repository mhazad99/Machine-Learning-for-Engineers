{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HiDgebwueLV"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcP7bCh-MeuW",
        "outputId": "aa733d72-9cac-4993-e6d0-be09f5971458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/ML3\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/My Drive/ML3/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Update the CNNModel to include dropout\n",
        "class CNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN1, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        label = self.targets[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Define the transformation\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.RandomRotation(30),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "])\n",
        "\n",
        "# Hyper parameters\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Load custom dataset\n",
        "data = pickle.load(open('Train.pkl', 'rb'))\n",
        "targets = pd.read_csv('Train_labels.csv', usecols=['class']).to_numpy().flatten()\n",
        "data = torch.from_numpy(data)\n",
        "targets = torch.from_numpy(targets)\n",
        "dataset = CustomDataset(data, targets, transform=data_transforms)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(split_ratio * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Read test data into a Dataset\n",
        "test_data = pickle.load(open('Test.pkl', 'rb'))\n",
        "test_data = torch.from_numpy(test_data)\n",
        "test_targets = torch.zeros(len(test_data), dtype=torch.long)\n",
        "test_dataset = CustomDataset(test_data, test_targets, transform=data_transforms)\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN1().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Save the true labels and predictions for confusion matrix calculation\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Accuracy of the model on the validation images: {100 * correct / total:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI7039F_Bts2",
        "outputId": "d990590a-ad6a-44e8-8fa9-44c09f1f99a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Step [100/3000], Loss: 2.1510\n",
            "Epoch [1/15], Step [200/3000], Loss: 1.7114\n",
            "Epoch [1/15], Step [300/3000], Loss: 1.8555\n",
            "Epoch [1/15], Step [400/3000], Loss: 1.0362\n",
            "Epoch [1/15], Step [500/3000], Loss: 0.9432\n",
            "Epoch [1/15], Step [600/3000], Loss: 1.1751\n",
            "Epoch [1/15], Step [700/3000], Loss: 1.5285\n",
            "Epoch [1/15], Step [800/3000], Loss: 1.3614\n",
            "Epoch [1/15], Step [900/3000], Loss: 1.1652\n",
            "Epoch [1/15], Step [1000/3000], Loss: 0.5864\n",
            "Epoch [1/15], Step [1100/3000], Loss: 1.1708\n",
            "Epoch [1/15], Step [1200/3000], Loss: 1.1750\n",
            "Epoch [1/15], Step [1300/3000], Loss: 0.7567\n",
            "Epoch [1/15], Step [1400/3000], Loss: 1.2690\n",
            "Epoch [1/15], Step [1500/3000], Loss: 1.3158\n",
            "Epoch [1/15], Step [1600/3000], Loss: 0.5160\n",
            "Epoch [1/15], Step [1700/3000], Loss: 0.4046\n",
            "Epoch [1/15], Step [1800/3000], Loss: 0.8143\n",
            "Epoch [1/15], Step [1900/3000], Loss: 0.4616\n",
            "Epoch [1/15], Step [2000/3000], Loss: 0.6841\n",
            "Epoch [1/15], Step [2100/3000], Loss: 0.5993\n",
            "Epoch [1/15], Step [2200/3000], Loss: 0.5642\n",
            "Epoch [1/15], Step [2300/3000], Loss: 0.9629\n",
            "Epoch [1/15], Step [2400/3000], Loss: 0.9898\n",
            "Epoch [1/15], Step [2500/3000], Loss: 0.5378\n",
            "Epoch [1/15], Step [2600/3000], Loss: 0.6072\n",
            "Epoch [1/15], Step [2700/3000], Loss: 0.7162\n",
            "Epoch [1/15], Step [2800/3000], Loss: 0.8432\n",
            "Epoch [1/15], Step [2900/3000], Loss: 0.4721\n",
            "Epoch [1/15], Step [3000/3000], Loss: 0.1844\n",
            "Epoch [2/15], Step [100/3000], Loss: 0.3405\n",
            "Epoch [2/15], Step [200/3000], Loss: 0.7670\n",
            "Epoch [2/15], Step [300/3000], Loss: 0.2125\n",
            "Epoch [2/15], Step [400/3000], Loss: 0.4886\n",
            "Epoch [2/15], Step [500/3000], Loss: 0.2883\n",
            "Epoch [2/15], Step [600/3000], Loss: 0.8105\n",
            "Epoch [2/15], Step [700/3000], Loss: 0.2065\n",
            "Epoch [2/15], Step [800/3000], Loss: 0.3379\n",
            "Epoch [2/15], Step [900/3000], Loss: 0.1729\n",
            "Epoch [2/15], Step [1000/3000], Loss: 0.2690\n",
            "Epoch [2/15], Step [1100/3000], Loss: 0.2792\n",
            "Epoch [2/15], Step [1200/3000], Loss: 0.4669\n",
            "Epoch [2/15], Step [1300/3000], Loss: 0.1489\n",
            "Epoch [2/15], Step [1400/3000], Loss: 0.4359\n",
            "Epoch [2/15], Step [1500/3000], Loss: 0.4844\n",
            "Epoch [2/15], Step [1600/3000], Loss: 0.6235\n",
            "Epoch [2/15], Step [1700/3000], Loss: 0.0693\n",
            "Epoch [2/15], Step [1800/3000], Loss: 0.1613\n",
            "Epoch [2/15], Step [1900/3000], Loss: 0.1112\n",
            "Epoch [2/15], Step [2000/3000], Loss: 0.3741\n",
            "Epoch [2/15], Step [2100/3000], Loss: 0.0760\n",
            "Epoch [2/15], Step [2200/3000], Loss: 0.1651\n",
            "Epoch [2/15], Step [2300/3000], Loss: 0.6725\n",
            "Epoch [2/15], Step [2400/3000], Loss: 0.3482\n",
            "Epoch [2/15], Step [2500/3000], Loss: 0.1944\n",
            "Epoch [2/15], Step [2600/3000], Loss: 0.4642\n",
            "Epoch [2/15], Step [2700/3000], Loss: 0.2225\n",
            "Epoch [2/15], Step [2800/3000], Loss: 0.4715\n",
            "Epoch [2/15], Step [2900/3000], Loss: 0.3380\n",
            "Epoch [2/15], Step [3000/3000], Loss: 0.1156\n",
            "Epoch [3/15], Step [100/3000], Loss: 0.3677\n",
            "Epoch [3/15], Step [200/3000], Loss: 0.5626\n",
            "Epoch [3/15], Step [300/3000], Loss: 0.1506\n",
            "Epoch [3/15], Step [400/3000], Loss: 0.6359\n",
            "Epoch [3/15], Step [500/3000], Loss: 0.2666\n",
            "Epoch [3/15], Step [600/3000], Loss: 0.4113\n",
            "Epoch [3/15], Step [700/3000], Loss: 0.1041\n",
            "Epoch [3/15], Step [800/3000], Loss: 0.1733\n",
            "Epoch [3/15], Step [900/3000], Loss: 0.1566\n",
            "Epoch [3/15], Step [1000/3000], Loss: 0.0224\n",
            "Epoch [3/15], Step [1100/3000], Loss: 0.3293\n",
            "Epoch [3/15], Step [1200/3000], Loss: 0.1236\n",
            "Epoch [3/15], Step [1300/3000], Loss: 0.0687\n",
            "Epoch [3/15], Step [1400/3000], Loss: 0.5305\n",
            "Epoch [3/15], Step [1500/3000], Loss: 0.0505\n",
            "Epoch [3/15], Step [1600/3000], Loss: 0.0211\n",
            "Epoch [3/15], Step [1700/3000], Loss: 0.1306\n",
            "Epoch [3/15], Step [1800/3000], Loss: 0.3135\n",
            "Epoch [3/15], Step [1900/3000], Loss: 0.0810\n",
            "Epoch [3/15], Step [2000/3000], Loss: 0.5398\n",
            "Epoch [3/15], Step [2100/3000], Loss: 0.4018\n",
            "Epoch [3/15], Step [2200/3000], Loss: 0.1514\n",
            "Epoch [3/15], Step [2300/3000], Loss: 0.6987\n",
            "Epoch [3/15], Step [2400/3000], Loss: 0.3876\n",
            "Epoch [3/15], Step [2500/3000], Loss: 0.5547\n",
            "Epoch [3/15], Step [2600/3000], Loss: 0.0516\n",
            "Epoch [3/15], Step [2700/3000], Loss: 0.3508\n",
            "Epoch [3/15], Step [2800/3000], Loss: 0.6874\n",
            "Epoch [3/15], Step [2900/3000], Loss: 0.3131\n",
            "Epoch [3/15], Step [3000/3000], Loss: 0.2292\n",
            "Epoch [4/15], Step [100/3000], Loss: 0.1431\n",
            "Epoch [4/15], Step [200/3000], Loss: 0.1976\n",
            "Epoch [4/15], Step [300/3000], Loss: 0.0289\n",
            "Epoch [4/15], Step [400/3000], Loss: 0.1319\n",
            "Epoch [4/15], Step [500/3000], Loss: 0.3131\n",
            "Epoch [4/15], Step [600/3000], Loss: 0.2351\n",
            "Epoch [4/15], Step [700/3000], Loss: 0.0365\n",
            "Epoch [4/15], Step [800/3000], Loss: 0.0694\n",
            "Epoch [4/15], Step [900/3000], Loss: 0.1838\n",
            "Epoch [4/15], Step [1000/3000], Loss: 0.1066\n",
            "Epoch [4/15], Step [1100/3000], Loss: 0.0256\n",
            "Epoch [4/15], Step [1200/3000], Loss: 0.0254\n",
            "Epoch [4/15], Step [1300/3000], Loss: 0.1055\n",
            "Epoch [4/15], Step [1400/3000], Loss: 0.2051\n",
            "Epoch [4/15], Step [1500/3000], Loss: 0.0683\n",
            "Epoch [4/15], Step [1600/3000], Loss: 0.3813\n",
            "Epoch [4/15], Step [1700/3000], Loss: 0.3202\n",
            "Epoch [4/15], Step [1800/3000], Loss: 0.3076\n",
            "Epoch [4/15], Step [1900/3000], Loss: 0.3027\n",
            "Epoch [4/15], Step [2000/3000], Loss: 0.0916\n",
            "Epoch [4/15], Step [2100/3000], Loss: 0.1249\n",
            "Epoch [4/15], Step [2200/3000], Loss: 0.0404\n",
            "Epoch [4/15], Step [2300/3000], Loss: 0.1475\n",
            "Epoch [4/15], Step [2400/3000], Loss: 0.0378\n",
            "Epoch [4/15], Step [2500/3000], Loss: 0.3305\n",
            "Epoch [4/15], Step [2600/3000], Loss: 0.0260\n",
            "Epoch [4/15], Step [2700/3000], Loss: 0.5664\n",
            "Epoch [4/15], Step [2800/3000], Loss: 0.0324\n",
            "Epoch [4/15], Step [2900/3000], Loss: 0.5171\n",
            "Epoch [4/15], Step [3000/3000], Loss: 0.6747\n",
            "Epoch [5/15], Step [100/3000], Loss: 0.1656\n",
            "Epoch [5/15], Step [200/3000], Loss: 0.0471\n",
            "Epoch [5/15], Step [300/3000], Loss: 0.2017\n",
            "Epoch [5/15], Step [400/3000], Loss: 0.2634\n",
            "Epoch [5/15], Step [500/3000], Loss: 0.1430\n",
            "Epoch [5/15], Step [600/3000], Loss: 0.1217\n",
            "Epoch [5/15], Step [700/3000], Loss: 0.0539\n",
            "Epoch [5/15], Step [800/3000], Loss: 0.0179\n",
            "Epoch [5/15], Step [900/3000], Loss: 0.0441\n",
            "Epoch [5/15], Step [1000/3000], Loss: 0.3085\n",
            "Epoch [5/15], Step [1100/3000], Loss: 0.2362\n",
            "Epoch [5/15], Step [1200/3000], Loss: 0.2951\n",
            "Epoch [5/15], Step [1300/3000], Loss: 0.0080\n",
            "Epoch [5/15], Step [1400/3000], Loss: 0.0657\n",
            "Epoch [5/15], Step [1500/3000], Loss: 0.0735\n",
            "Epoch [5/15], Step [1600/3000], Loss: 0.0045\n",
            "Epoch [5/15], Step [1700/3000], Loss: 0.1776\n",
            "Epoch [5/15], Step [1800/3000], Loss: 0.1343\n",
            "Epoch [5/15], Step [1900/3000], Loss: 0.5843\n",
            "Epoch [5/15], Step [2000/3000], Loss: 0.4220\n",
            "Epoch [5/15], Step [2100/3000], Loss: 0.0278\n",
            "Epoch [5/15], Step [2200/3000], Loss: 0.3729\n",
            "Epoch [5/15], Step [2300/3000], Loss: 0.0348\n",
            "Epoch [5/15], Step [2400/3000], Loss: 0.2415\n",
            "Epoch [5/15], Step [2500/3000], Loss: 0.0161\n",
            "Epoch [5/15], Step [2600/3000], Loss: 0.0382\n",
            "Epoch [5/15], Step [2700/3000], Loss: 0.0846\n",
            "Epoch [5/15], Step [2800/3000], Loss: 0.0571\n",
            "Epoch [5/15], Step [2900/3000], Loss: 0.0458\n",
            "Epoch [5/15], Step [3000/3000], Loss: 0.1487\n",
            "Epoch [6/15], Step [100/3000], Loss: 0.1121\n",
            "Epoch [6/15], Step [200/3000], Loss: 0.2055\n",
            "Epoch [6/15], Step [300/3000], Loss: 0.0365\n",
            "Epoch [6/15], Step [400/3000], Loss: 0.0294\n",
            "Epoch [6/15], Step [500/3000], Loss: 0.0245\n",
            "Epoch [6/15], Step [600/3000], Loss: 0.1418\n",
            "Epoch [6/15], Step [700/3000], Loss: 0.1753\n",
            "Epoch [6/15], Step [800/3000], Loss: 0.1263\n",
            "Epoch [6/15], Step [900/3000], Loss: 0.0188\n",
            "Epoch [6/15], Step [1000/3000], Loss: 0.1190\n",
            "Epoch [6/15], Step [1100/3000], Loss: 0.0802\n",
            "Epoch [6/15], Step [1200/3000], Loss: 0.1859\n",
            "Epoch [6/15], Step [1300/3000], Loss: 0.1578\n",
            "Epoch [6/15], Step [1400/3000], Loss: 0.0108\n",
            "Epoch [6/15], Step [1500/3000], Loss: 0.0598\n",
            "Epoch [6/15], Step [1600/3000], Loss: 0.0750\n",
            "Epoch [6/15], Step [1700/3000], Loss: 0.2906\n",
            "Epoch [6/15], Step [1800/3000], Loss: 0.0208\n",
            "Epoch [6/15], Step [1900/3000], Loss: 0.2596\n",
            "Epoch [6/15], Step [2000/3000], Loss: 0.0077\n",
            "Epoch [6/15], Step [2100/3000], Loss: 0.0441\n",
            "Epoch [6/15], Step [2200/3000], Loss: 0.0452\n",
            "Epoch [6/15], Step [2300/3000], Loss: 0.1535\n",
            "Epoch [6/15], Step [2400/3000], Loss: 0.2892\n",
            "Epoch [6/15], Step [2500/3000], Loss: 0.1786\n",
            "Epoch [6/15], Step [2600/3000], Loss: 0.2491\n",
            "Epoch [6/15], Step [2700/3000], Loss: 0.3567\n",
            "Epoch [6/15], Step [2800/3000], Loss: 0.3093\n",
            "Epoch [6/15], Step [2900/3000], Loss: 0.0146\n",
            "Epoch [6/15], Step [3000/3000], Loss: 0.2201\n",
            "Epoch [7/15], Step [100/3000], Loss: 0.1772\n",
            "Epoch [7/15], Step [200/3000], Loss: 0.0090\n",
            "Epoch [7/15], Step [300/3000], Loss: 0.0180\n",
            "Epoch [7/15], Step [400/3000], Loss: 0.0064\n",
            "Epoch [7/15], Step [500/3000], Loss: 0.4649\n",
            "Epoch [7/15], Step [600/3000], Loss: 0.0366\n",
            "Epoch [7/15], Step [700/3000], Loss: 0.0850\n",
            "Epoch [7/15], Step [800/3000], Loss: 0.0044\n",
            "Epoch [7/15], Step [900/3000], Loss: 0.0040\n",
            "Epoch [7/15], Step [1000/3000], Loss: 0.0517\n",
            "Epoch [7/15], Step [1100/3000], Loss: 0.2193\n",
            "Epoch [7/15], Step [1200/3000], Loss: 0.1551\n",
            "Epoch [7/15], Step [1300/3000], Loss: 0.1515\n",
            "Epoch [7/15], Step [1400/3000], Loss: 0.1403\n",
            "Epoch [7/15], Step [1500/3000], Loss: 0.0370\n",
            "Epoch [7/15], Step [1600/3000], Loss: 0.0183\n",
            "Epoch [7/15], Step [1700/3000], Loss: 0.0053\n",
            "Epoch [7/15], Step [1800/3000], Loss: 0.0111\n",
            "Epoch [7/15], Step [1900/3000], Loss: 0.0148\n",
            "Epoch [7/15], Step [2000/3000], Loss: 0.3002\n",
            "Epoch [7/15], Step [2100/3000], Loss: 0.0098\n",
            "Epoch [7/15], Step [2200/3000], Loss: 0.0491\n",
            "Epoch [7/15], Step [2300/3000], Loss: 0.2445\n",
            "Epoch [7/15], Step [2400/3000], Loss: 0.0083\n",
            "Epoch [7/15], Step [2500/3000], Loss: 0.0168\n",
            "Epoch [7/15], Step [2600/3000], Loss: 0.0069\n",
            "Epoch [7/15], Step [2700/3000], Loss: 0.0390\n",
            "Epoch [7/15], Step [2800/3000], Loss: 0.0116\n",
            "Epoch [7/15], Step [2900/3000], Loss: 0.0087\n",
            "Epoch [7/15], Step [3000/3000], Loss: 0.1190\n",
            "Epoch [8/15], Step [100/3000], Loss: 0.1025\n",
            "Epoch [8/15], Step [200/3000], Loss: 0.0209\n",
            "Epoch [8/15], Step [300/3000], Loss: 0.0087\n",
            "Epoch [8/15], Step [400/3000], Loss: 0.0295\n",
            "Epoch [8/15], Step [500/3000], Loss: 0.0066\n",
            "Epoch [8/15], Step [600/3000], Loss: 0.0024\n",
            "Epoch [8/15], Step [700/3000], Loss: 0.0016\n",
            "Epoch [8/15], Step [800/3000], Loss: 0.0036\n",
            "Epoch [8/15], Step [900/3000], Loss: 0.0128\n",
            "Epoch [8/15], Step [1000/3000], Loss: 0.1386\n",
            "Epoch [8/15], Step [1100/3000], Loss: 0.0129\n",
            "Epoch [8/15], Step [1200/3000], Loss: 0.0800\n",
            "Epoch [8/15], Step [1300/3000], Loss: 0.0193\n",
            "Epoch [8/15], Step [1400/3000], Loss: 0.0525\n",
            "Epoch [8/15], Step [1500/3000], Loss: 0.1144\n",
            "Epoch [8/15], Step [1600/3000], Loss: 0.0072\n",
            "Epoch [8/15], Step [1700/3000], Loss: 0.0140\n",
            "Epoch [8/15], Step [1800/3000], Loss: 0.0189\n",
            "Epoch [8/15], Step [1900/3000], Loss: 0.0131\n",
            "Epoch [8/15], Step [2000/3000], Loss: 0.1221\n",
            "Epoch [8/15], Step [2100/3000], Loss: 0.0429\n",
            "Epoch [8/15], Step [2200/3000], Loss: 0.0047\n",
            "Epoch [8/15], Step [2300/3000], Loss: 0.0192\n",
            "Epoch [8/15], Step [2400/3000], Loss: 0.1264\n",
            "Epoch [8/15], Step [2500/3000], Loss: 0.1229\n",
            "Epoch [8/15], Step [2600/3000], Loss: 0.5038\n",
            "Epoch [8/15], Step [2700/3000], Loss: 0.0231\n",
            "Epoch [8/15], Step [2800/3000], Loss: 0.2797\n",
            "Epoch [8/15], Step [2900/3000], Loss: 0.0284\n",
            "Epoch [8/15], Step [3000/3000], Loss: 0.3660\n",
            "Epoch [9/15], Step [100/3000], Loss: 0.2682\n",
            "Epoch [9/15], Step [200/3000], Loss: 0.0222\n",
            "Epoch [9/15], Step [300/3000], Loss: 0.0822\n",
            "Epoch [9/15], Step [400/3000], Loss: 0.0901\n",
            "Epoch [9/15], Step [500/3000], Loss: 0.1892\n",
            "Epoch [9/15], Step [600/3000], Loss: 0.0100\n",
            "Epoch [9/15], Step [700/3000], Loss: 0.0092\n",
            "Epoch [9/15], Step [800/3000], Loss: 0.0810\n",
            "Epoch [9/15], Step [900/3000], Loss: 0.0043\n",
            "Epoch [9/15], Step [1000/3000], Loss: 0.0065\n",
            "Epoch [9/15], Step [1100/3000], Loss: 0.0105\n",
            "Epoch [9/15], Step [1200/3000], Loss: 0.0186\n",
            "Epoch [9/15], Step [1300/3000], Loss: 0.0146\n",
            "Epoch [9/15], Step [1400/3000], Loss: 0.1508\n",
            "Epoch [9/15], Step [1500/3000], Loss: 0.1228\n",
            "Epoch [9/15], Step [1600/3000], Loss: 0.0016\n",
            "Epoch [9/15], Step [1700/3000], Loss: 0.0088\n",
            "Epoch [9/15], Step [1800/3000], Loss: 0.2207\n",
            "Epoch [9/15], Step [1900/3000], Loss: 0.0042\n",
            "Epoch [9/15], Step [2000/3000], Loss: 0.0870\n",
            "Epoch [9/15], Step [2100/3000], Loss: 0.0044\n",
            "Epoch [9/15], Step [2200/3000], Loss: 0.0041\n",
            "Epoch [9/15], Step [2300/3000], Loss: 0.0438\n",
            "Epoch [9/15], Step [2400/3000], Loss: 0.0629\n",
            "Epoch [9/15], Step [2500/3000], Loss: 0.2450\n",
            "Epoch [9/15], Step [2600/3000], Loss: 0.0228\n",
            "Epoch [9/15], Step [2700/3000], Loss: 0.0041\n",
            "Epoch [9/15], Step [2800/3000], Loss: 0.0015\n",
            "Epoch [9/15], Step [2900/3000], Loss: 0.0472\n",
            "Epoch [9/15], Step [3000/3000], Loss: 0.0009\n",
            "Epoch [10/15], Step [100/3000], Loss: 0.0089\n",
            "Epoch [10/15], Step [200/3000], Loss: 0.0142\n",
            "Epoch [10/15], Step [300/3000], Loss: 0.0292\n",
            "Epoch [10/15], Step [400/3000], Loss: 0.0054\n",
            "Epoch [10/15], Step [500/3000], Loss: 0.2082\n",
            "Epoch [10/15], Step [600/3000], Loss: 0.0015\n",
            "Epoch [10/15], Step [700/3000], Loss: 0.0002\n",
            "Epoch [10/15], Step [800/3000], Loss: 0.0611\n",
            "Epoch [10/15], Step [900/3000], Loss: 0.0458\n",
            "Epoch [10/15], Step [1000/3000], Loss: 0.0343\n",
            "Epoch [10/15], Step [1100/3000], Loss: 0.0392\n",
            "Epoch [10/15], Step [1200/3000], Loss: 0.1208\n",
            "Epoch [10/15], Step [1300/3000], Loss: 0.0003\n",
            "Epoch [10/15], Step [1400/3000], Loss: 0.0043\n",
            "Epoch [10/15], Step [1500/3000], Loss: 0.0103\n",
            "Epoch [10/15], Step [1600/3000], Loss: 0.2794\n",
            "Epoch [10/15], Step [1700/3000], Loss: 0.0811\n",
            "Epoch [10/15], Step [1800/3000], Loss: 0.0962\n",
            "Epoch [10/15], Step [1900/3000], Loss: 0.1648\n",
            "Epoch [10/15], Step [2000/3000], Loss: 0.0445\n",
            "Epoch [10/15], Step [2100/3000], Loss: 0.0113\n",
            "Epoch [10/15], Step [2200/3000], Loss: 0.3002\n",
            "Epoch [10/15], Step [2300/3000], Loss: 0.0657\n",
            "Epoch [10/15], Step [2400/3000], Loss: 0.0114\n",
            "Epoch [10/15], Step [2500/3000], Loss: 0.0139\n",
            "Epoch [10/15], Step [2600/3000], Loss: 0.0630\n",
            "Epoch [10/15], Step [2700/3000], Loss: 0.0717\n",
            "Epoch [10/15], Step [2800/3000], Loss: 0.0067\n",
            "Epoch [10/15], Step [2900/3000], Loss: 0.0148\n",
            "Epoch [10/15], Step [3000/3000], Loss: 0.1704\n",
            "Epoch [11/15], Step [100/3000], Loss: 0.0450\n",
            "Epoch [11/15], Step [200/3000], Loss: 0.0161\n",
            "Epoch [11/15], Step [300/3000], Loss: 0.0142\n",
            "Epoch [11/15], Step [400/3000], Loss: 0.0112\n",
            "Epoch [11/15], Step [500/3000], Loss: 0.1337\n",
            "Epoch [11/15], Step [600/3000], Loss: 0.0132\n",
            "Epoch [11/15], Step [700/3000], Loss: 0.0227\n",
            "Epoch [11/15], Step [800/3000], Loss: 0.0065\n",
            "Epoch [11/15], Step [900/3000], Loss: 0.0228\n",
            "Epoch [11/15], Step [1000/3000], Loss: 0.0005\n",
            "Epoch [11/15], Step [1100/3000], Loss: 0.0626\n",
            "Epoch [11/15], Step [1200/3000], Loss: 0.0046\n",
            "Epoch [11/15], Step [1300/3000], Loss: 0.0028\n",
            "Epoch [11/15], Step [1400/3000], Loss: 0.2151\n",
            "Epoch [11/15], Step [1500/3000], Loss: 0.0024\n",
            "Epoch [11/15], Step [1600/3000], Loss: 0.1011\n",
            "Epoch [11/15], Step [1700/3000], Loss: 0.0741\n",
            "Epoch [11/15], Step [1800/3000], Loss: 0.0022\n",
            "Epoch [11/15], Step [1900/3000], Loss: 0.0389\n",
            "Epoch [11/15], Step [2000/3000], Loss: 0.0172\n",
            "Epoch [11/15], Step [2100/3000], Loss: 0.0491\n",
            "Epoch [11/15], Step [2200/3000], Loss: 0.4493\n",
            "Epoch [11/15], Step [2300/3000], Loss: 0.0038\n",
            "Epoch [11/15], Step [2400/3000], Loss: 0.1831\n",
            "Epoch [11/15], Step [2500/3000], Loss: 0.2133\n",
            "Epoch [11/15], Step [2600/3000], Loss: 0.2840\n",
            "Epoch [11/15], Step [2700/3000], Loss: 0.0013\n",
            "Epoch [11/15], Step [2800/3000], Loss: 0.0045\n",
            "Epoch [11/15], Step [2900/3000], Loss: 0.0005\n",
            "Epoch [11/15], Step [3000/3000], Loss: 0.0126\n",
            "Epoch [12/15], Step [100/3000], Loss: 0.0111\n",
            "Epoch [12/15], Step [200/3000], Loss: 0.0035\n",
            "Epoch [12/15], Step [300/3000], Loss: 0.0020\n",
            "Epoch [12/15], Step [400/3000], Loss: 0.0007\n",
            "Epoch [12/15], Step [500/3000], Loss: 0.0746\n",
            "Epoch [12/15], Step [600/3000], Loss: 0.0018\n",
            "Epoch [12/15], Step [700/3000], Loss: 0.0017\n",
            "Epoch [12/15], Step [800/3000], Loss: 0.0546\n",
            "Epoch [12/15], Step [900/3000], Loss: 0.0131\n",
            "Epoch [12/15], Step [1000/3000], Loss: 0.0081\n",
            "Epoch [12/15], Step [1100/3000], Loss: 0.0035\n",
            "Epoch [12/15], Step [1200/3000], Loss: 0.0339\n",
            "Epoch [12/15], Step [1300/3000], Loss: 0.1275\n",
            "Epoch [12/15], Step [1400/3000], Loss: 0.0055\n",
            "Epoch [12/15], Step [1500/3000], Loss: 0.0064\n",
            "Epoch [12/15], Step [1600/3000], Loss: 0.0136\n",
            "Epoch [12/15], Step [1700/3000], Loss: 0.0039\n",
            "Epoch [12/15], Step [1800/3000], Loss: 0.0030\n",
            "Epoch [12/15], Step [1900/3000], Loss: 0.0036\n",
            "Epoch [12/15], Step [2000/3000], Loss: 0.0010\n",
            "Epoch [12/15], Step [2100/3000], Loss: 0.0112\n",
            "Epoch [12/15], Step [2200/3000], Loss: 0.0025\n",
            "Epoch [12/15], Step [2300/3000], Loss: 0.0091\n",
            "Epoch [12/15], Step [2400/3000], Loss: 0.1031\n",
            "Epoch [12/15], Step [2500/3000], Loss: 0.0049\n",
            "Epoch [12/15], Step [2600/3000], Loss: 0.0027\n",
            "Epoch [12/15], Step [2700/3000], Loss: 0.0102\n",
            "Epoch [12/15], Step [2800/3000], Loss: 0.0549\n",
            "Epoch [12/15], Step [2900/3000], Loss: 0.3704\n",
            "Epoch [12/15], Step [3000/3000], Loss: 0.0670\n",
            "Epoch [13/15], Step [100/3000], Loss: 0.0010\n",
            "Epoch [13/15], Step [200/3000], Loss: 0.0049\n",
            "Epoch [13/15], Step [300/3000], Loss: 0.0005\n",
            "Epoch [13/15], Step [400/3000], Loss: 0.0008\n",
            "Epoch [13/15], Step [500/3000], Loss: 0.0123\n",
            "Epoch [13/15], Step [600/3000], Loss: 0.0410\n",
            "Epoch [13/15], Step [700/3000], Loss: 0.3851\n",
            "Epoch [13/15], Step [800/3000], Loss: 0.0058\n",
            "Epoch [13/15], Step [900/3000], Loss: 0.0187\n",
            "Epoch [13/15], Step [1000/3000], Loss: 0.0341\n",
            "Epoch [13/15], Step [1100/3000], Loss: 0.0436\n",
            "Epoch [13/15], Step [1200/3000], Loss: 0.0050\n",
            "Epoch [13/15], Step [1300/3000], Loss: 0.0004\n",
            "Epoch [13/15], Step [1400/3000], Loss: 0.2012\n",
            "Epoch [13/15], Step [1500/3000], Loss: 0.0069\n",
            "Epoch [13/15], Step [1600/3000], Loss: 0.0331\n",
            "Epoch [13/15], Step [1700/3000], Loss: 0.0030\n",
            "Epoch [13/15], Step [1800/3000], Loss: 0.0362\n",
            "Epoch [13/15], Step [1900/3000], Loss: 0.0453\n",
            "Epoch [13/15], Step [2000/3000], Loss: 0.0276\n",
            "Epoch [13/15], Step [2100/3000], Loss: 0.0837\n",
            "Epoch [13/15], Step [2200/3000], Loss: 0.0014\n",
            "Epoch [13/15], Step [2300/3000], Loss: 0.0165\n",
            "Epoch [13/15], Step [2400/3000], Loss: 0.0155\n",
            "Epoch [13/15], Step [2500/3000], Loss: 0.0682\n",
            "Epoch [13/15], Step [2600/3000], Loss: 0.0235\n",
            "Epoch [13/15], Step [2700/3000], Loss: 0.0004\n",
            "Epoch [13/15], Step [2800/3000], Loss: 0.0282\n",
            "Epoch [13/15], Step [2900/3000], Loss: 0.0008\n",
            "Epoch [13/15], Step [3000/3000], Loss: 0.0073\n",
            "Epoch [14/15], Step [100/3000], Loss: 0.0101\n",
            "Epoch [14/15], Step [200/3000], Loss: 0.0093\n",
            "Epoch [14/15], Step [300/3000], Loss: 0.0226\n",
            "Epoch [14/15], Step [400/3000], Loss: 0.0561\n",
            "Epoch [14/15], Step [500/3000], Loss: 0.0083\n",
            "Epoch [14/15], Step [600/3000], Loss: 0.0025\n",
            "Epoch [14/15], Step [700/3000], Loss: 0.0135\n",
            "Epoch [14/15], Step [800/3000], Loss: 0.1456\n",
            "Epoch [14/15], Step [900/3000], Loss: 0.4836\n",
            "Epoch [14/15], Step [1000/3000], Loss: 0.0436\n",
            "Epoch [14/15], Step [1100/3000], Loss: 0.0850\n",
            "Epoch [14/15], Step [1200/3000], Loss: 0.0075\n",
            "Epoch [14/15], Step [1300/3000], Loss: 0.0020\n",
            "Epoch [14/15], Step [1400/3000], Loss: 0.0075\n",
            "Epoch [14/15], Step [1500/3000], Loss: 0.0110\n",
            "Epoch [14/15], Step [1600/3000], Loss: 0.1305\n",
            "Epoch [14/15], Step [1700/3000], Loss: 0.0080\n",
            "Epoch [14/15], Step [1800/3000], Loss: 0.0155\n",
            "Epoch [14/15], Step [1900/3000], Loss: 0.0398\n",
            "Epoch [14/15], Step [2000/3000], Loss: 0.0012\n",
            "Epoch [14/15], Step [2100/3000], Loss: 0.1050\n",
            "Epoch [14/15], Step [2200/3000], Loss: 0.0012\n",
            "Epoch [14/15], Step [2300/3000], Loss: 0.2584\n",
            "Epoch [14/15], Step [2400/3000], Loss: 0.2127\n",
            "Epoch [14/15], Step [2500/3000], Loss: 0.0157\n",
            "Epoch [14/15], Step [2600/3000], Loss: 0.0102\n",
            "Epoch [14/15], Step [2700/3000], Loss: 0.0208\n",
            "Epoch [14/15], Step [2800/3000], Loss: 0.0299\n",
            "Epoch [14/15], Step [2900/3000], Loss: 0.0043\n",
            "Epoch [14/15], Step [3000/3000], Loss: 0.0021\n",
            "Epoch [15/15], Step [100/3000], Loss: 0.0007\n",
            "Epoch [15/15], Step [200/3000], Loss: 0.0873\n",
            "Epoch [15/15], Step [300/3000], Loss: 0.0165\n",
            "Epoch [15/15], Step [400/3000], Loss: 0.0008\n",
            "Epoch [15/15], Step [500/3000], Loss: 0.0085\n",
            "Epoch [15/15], Step [600/3000], Loss: 0.0010\n",
            "Epoch [15/15], Step [700/3000], Loss: 0.1502\n",
            "Epoch [15/15], Step [800/3000], Loss: 0.0158\n",
            "Epoch [15/15], Step [900/3000], Loss: 0.0017\n",
            "Epoch [15/15], Step [1000/3000], Loss: 0.0008\n",
            "Epoch [15/15], Step [1100/3000], Loss: 0.0438\n",
            "Epoch [15/15], Step [1200/3000], Loss: 0.0004\n",
            "Epoch [15/15], Step [1300/3000], Loss: 0.0430\n",
            "Epoch [15/15], Step [1400/3000], Loss: 0.0035\n",
            "Epoch [15/15], Step [1500/3000], Loss: 0.0014\n",
            "Epoch [15/15], Step [1600/3000], Loss: 0.0029\n",
            "Epoch [15/15], Step [1700/3000], Loss: 0.0330\n",
            "Epoch [15/15], Step [1800/3000], Loss: 0.0007\n",
            "Epoch [15/15], Step [1900/3000], Loss: 0.0233\n",
            "Epoch [15/15], Step [2000/3000], Loss: 0.1156\n",
            "Epoch [15/15], Step [2100/3000], Loss: 0.0014\n",
            "Epoch [15/15], Step [2200/3000], Loss: 0.1172\n",
            "Epoch [15/15], Step [2300/3000], Loss: 0.0362\n",
            "Epoch [15/15], Step [2400/3000], Loss: 0.0010\n",
            "Epoch [15/15], Step [2500/3000], Loss: 0.3324\n",
            "Epoch [15/15], Step [2600/3000], Loss: 0.0219\n",
            "Epoch [15/15], Step [2700/3000], Loss: 0.0025\n",
            "Epoch [15/15], Step [2800/3000], Loss: 0.0047\n",
            "Epoch [15/15], Step [2900/3000], Loss: 0.4947\n",
            "Epoch [15/15], Step [3000/3000], Loss: 0.2451\n",
            "Confusion Matrix:\n",
            "[[1133    0    0    0   16    2    3    4    4    2]\n",
            " [   1 1122    3    2    5    4    6    6    9    9]\n",
            " [   2    9 1090   17   12    5   15    8   13    8]\n",
            " [   1    3    0 1145    8   18    5    6    7    1]\n",
            " [  14    2    2    4 1160    3    5   10    3   10]\n",
            " [   5    7    6   14   13 1146    2    2    4    3]\n",
            " [   3   24   22    1   14    7 1139   13   13    1]\n",
            " [   1    1    1    2    3    0    6 1181    0    4]\n",
            " [   5    3    3    5   16    2    3    4 1172    1]\n",
            " [   5    5    4    3    3    3    1    8    5 1194]]\n",
            "Accuracy of the model on the validation images: 95.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test dataset\n",
        "test_pred = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs = data[0]\n",
        "        inputs=inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Save the predicted labels to a CSV file\n",
        "test_ids = [i for i in range(len(test_pred))]\n",
        "test_df = pd.DataFrame({'id': test_ids, 'class': test_pred})\n",
        "test_df.to_csv('test_pred.csv', index=False)"
      ],
      "metadata": {
        "id": "pikqABmS_qqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "files.download('test_preds.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "V0AmjlfN84tU",
        "outputId": "a8c897e0-8c49-4150-c447-95cf936ef072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_afe633e4-64ac-4058-a611-1582c5bf1024\", \"test_preds.csv\", 68899)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}